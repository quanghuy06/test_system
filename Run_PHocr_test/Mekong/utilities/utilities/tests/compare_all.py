# TOSHIBA - TSDV
# Team:             OCRPoc
# Author:           Le Duc Nam
# Email:            nam.leduc@toshiba-tsdv.com
# Date create:      20/11/2017
# Last update:      07/07/2107
# Editor:           Phung Dinh Tai
# Email:            tai.phungdinh@toshiba-tsdv.com
# Description:      Adhoc testing. Run compare test result.
import argparse
import os
import shutil
import sys
import traceback
import sys_path
sys_path.insert_sys_path()
from handlers.test_result_handler import TestResultHandler
from configs.compare_result import CompareResultConfig
from baseapi.file_access import write_json, remove_paths
from tests.lib_comparison.compare_runner import CompareRunner


class CompareAll(object):
    def __init__(self, test_folder, platform, test_result, output_file=None, output_folder=None):
        self.test_folder = test_folder
        self.platform = platform
        self.test_result = test_result
        if output_file:
            self.output_file = output_file
        else:
            self.output_file = CompareResultConfig.FILE_DEFAULT
        if output_folder:
            self.output_folder = output_folder
        else:
            self.output_folder = CompareResultConfig.FOLDER_DEFAULT

    def do_work(self):
        compare_results = self.execute_compare()
        # Check and remove old file
        if os.path.isfile(self.output_file):
            remove_paths(self.output_file)
        self.write_data(compare_results)

    def execute_compare(self):
        # Create a result folder
        if os.path.exists(self.output_folder):
            shutil.rmtree(self.output_folder)
        os.makedirs(self.output_folder)

        # Get test set
        test_result_handler = TestResultHandler(input_file=self.test_result,
                                                test_folder=self.test_folder)
        # Only compare test case which is not error
        # Inform error tests
        error_list = test_result_handler.get_list_error_tests()
        if error_list:
            msg = "INFORM: There are {0} error test cases that will not be compared: \n\t" \
                  "".format(len(error_list))
            for test_name in error_list:
                msg += test_name + ", "
            print msg[:-2] + "\n"

        # Get list of test cases that are not error. These also will be compared.
        test_set = sorted(test_result_handler.get_list_not_error_tests())

        # Check if test case directory exists
        not_founds = []
        real_test_set = []
        for test_id in test_set:
            test_path = os.path.join(self.test_folder, test_id)
            if not os.path.isdir(test_path):
                not_founds.append(test_id)
            else:
                real_test_set.append(test_id)
        # Notice test cases that can not found in test folder
        if not_founds:
            msg = "INFORM: Can not find {0} test cases in test folder {1}: \n\t" \
                  "".format(len(not_founds), self.test_folder)
            for test_id in not_founds:
                msg += test_id + ", "
            print msg[:-2] + "\n"

        print "\n*** COMPARE START ***"

        if real_test_set:

            runner = CompareRunner()

            results = {}
            index = 1
            for test_id in real_test_set:
                print('[' + str(index) + '/' + str(len(real_test_set)) +
                      '] Compare for testcase "' + str(test_id) + '"')
                index += 1
                try:
                    run_infor = runner.run(self.test_folder, test_id,
                                           self.platform, self.output_folder)
                    results[test_id] = run_infor
                except Exception as e:
                    print('-' * 60)
                    traceback.print_exc(file=sys.stdout)
                    print('-' * 60)
            print "*** COMPARE END ***"
            return results
        else:
            print "All test cases are error!"

    def write_data(self, compare_results):
        if compare_results:
            # Write test result to json file
            write_json(compare_results, self.output_file)
        else:
            write_json({}, self.output_file)


def parse_argument():
    parser = argparse.ArgumentParser(
                description='Compare output and reference of all test cases on test folder')
    parser.add_argument('-t', '--test_folder', required=True,
                        help='Folder contain test set')
    parser.add_argument('-r', '--result-json', required=True,
                        help='Path to test result json file which generated by run scripts')
    parser.add_argument('-o', '--output-file', default=CompareResultConfig.FILE_DEFAULT,
                        help='Compare output file to export in json format')
    parser.add_argument('-p', '--platform',
                        help='Platform want to compare')
    parser.add_argument('-f', '--output-folder', default=CompareResultConfig.FOLDER_DEFAULT,
                        help='Output result folder to export')
    return parser.parse_args()


def main():
    # Parse arguments
    args = parse_argument()
    cmp_executor = CompareAll(args.test_folder, args.platform, args.result_json)
    cmp_executor.do_work()


if __name__ == "__main__":
    main()
